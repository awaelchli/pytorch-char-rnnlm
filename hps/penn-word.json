{
    "train_corpus": "./data/penn/train.txt",
    "eval_corpus": "./data/penn/valid.txt",
    "test_corpus": "./data/penn/test.txt",
    "tokenization": "word",
    "vocab_file": "./model/penn-word/vocab_file.json",
    "save": "./model/penn-word/pytorchmodel",
    "emsize": 1500,
    "nhid": 1500,
    "nlayers": 2,
    "dropout": 0.65,
    "tied": true,
    "batch_size": 20,
    "bptt": 35,
    "clip": 0.25,
    "log_interval": 1000,
    "lr": 20.0,
    "epochs": 100,
    "cuda": true
}
